<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8"> 
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>【最終修正版】Holistic 高精度視線推定</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/control_utils/control_utils.js" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/holistic/holistic.js" crossorigin="anonymous"></script>
    <style>
        body { background-color: #111827; }
        .canvas-container {
            position: relative;
            width: 100%;
            max-width: 960px;
            margin: auto;
            border-radius: 0.5rem;
            overflow: hidden;
            background-color: #000;
        }
        #output_canvas {
            width: 100%;
            height: auto;
            display: block;
        }
    </style>
</head>
<body class="text-white font-sans">

    <div class="container mx-auto p-4 max-w-7xl">
        <div class="text-center mb-6">
            <h1 class="text-3xl md:text-4xl font-bold">【最終修正版】Holistic 高精度視線推定</h1>
            <p class="text-gray-400 mt-2" id="status">AIモデルを読み込み中...</p>
        </div>

        <div class="canvas-container shadow-2xl">
            <video id="input_video" loop style="display: none;"></video>
            <canvas id="output_canvas"></canvas>
        </div>

        <div class="flex flex-col sm:flex-row gap-4 max-w-md mx-auto mt-6">
            <button id="webcamButton" class="flex-1 bg-blue-600 hover:bg-blue-700 text-white font-bold py-3 px-4 rounded-lg transition-colors duration-300">Webカメラを開始</button>
            <label for="videoFileInput" class="flex-1 bg-green-600 hover:bg-green-700 text-white font-bold py-3 px-4 rounded-lg transition-colors duration-300 cursor-pointer text-center">
                動画ファイルを選択
            </label>
            <input type="file" id="videoFileInput" accept="video/*" class="hidden">
        </div>
    </div>

    <script type="module">
        const videoElement = document.getElementById('input_video');
        const canvasElement = document.getElementById('output_canvas');
        const canvasCtx = canvasElement.getContext('2d');
        const statusText = document.getElementById('status');
        const webcamButton = document.getElementById('webcamButton');
        const videoFileInput = document.getElementById('videoFileInput');

        let isMirrored = false;
        let activeStream = false;

        function onResults(results) {
            statusText.textContent = '姿勢を検出中...';

            const canvasWidth = results.image.width;
            const canvasHeight = results.image.height;
            canvasElement.width = canvasWidth;
            canvasElement.height = canvasHeight;

            canvasCtx.save();
            canvasCtx.clearRect(0, 0, canvasWidth, canvasHeight);

            if (isMirrored) {
                canvasCtx.translate(canvasWidth, 0);
                canvasCtx.scale(-1, 1);
            }
            
            canvasCtx.drawImage(results.image, 0, 0, canvasWidth, canvasHeight);
            
            if (results.poseLandmarks) {
                drawConnectors(canvasCtx, results.poseLandmarks, POSE_CONNECTIONS, { color: '#CCCCCC', lineWidth: 2 });
            }
            if(results.faceLandmarks) {
                 drawConnectors(canvasCtx, results.faceLandmarks, FACEMESH_TESSELATION, {color: '#4ade80', lineWidth: 1});
            }
            
            const gazeData = getGazeData(results);
            if (gazeData) {
                drawGazeIndicator(gazeData, canvasCtx);
            }
            
            canvasCtx.restore();
        }
        
        // ★★★ 修正箇所: 視線推定ロジックの全面見直し ★★★
        function getGazeData(results) {
            // 【優先1】顔が検出できている場合、Face Meshの鼻筋ベクトルを使う
            if (results.faceLandmarks) {
                const face = results.faceLandmarks;
                const noseTip = face[1];     // 鼻の先端
                const noseBridge = face[6];  // 鼻の付け根

                // ★修正点: 認識条件を緩和し、より積極的に高精度モードを利用
                if (noseTip.visibility > 0.6 && noseBridge.visibility > 0.6) {
                    const directionVector = {
                        x: noseTip.x - noseBridge.x,
                        y: noseTip.y - noseBridge.y
                    };
                    return { startPoint: noseTip, direction: directionVector };
                }
            }

            // 【優先2】顔が見えない場合、Poseの骨格情報で代用（バックアップ処理）
            if (results.poseLandmarks) {
                const pose = results.poseLandmarks;
                const nose = pose[0];
                const leftShoulder = pose[11];
                const rightShoulder = pose[12];
                const leftEar = pose[7];
                const rightEar = pose[8];

                if (nose.visibility > 0.5 && leftShoulder.visibility > 0.5 && rightShoulder.visibility > 0.5) {
                    const yaw = (leftShoulder.z - rightShoulder.z) * 2.5;

                    // ★修正点: バグのあったバックアップ処理を、以前の安定版ピッチ計算ロジックに修正
                    let pitchAngle = 0;
                    if (leftEar.visibility > 0.5 && rightEar.visibility > 0.5) {
                        const earMidY = (leftEar.y + rightEar.y) / 2;
                        const shoulderMidY = (leftShoulder.y + rightShoulder.y) / 2;
                        const headHeight = Math.abs(shoulderMidY - earMidY);
                        const rawPitch = earMidY - nose.y;
                        pitchAngle = (rawPitch / (headHeight + 0.01)) * 1.5;
                    }

                    // 角度から方向ベクトルを生成
                    const directionVector = {
                        x: -Math.sin(yaw),
                        y: -Math.tan(pitchAngle)
                    };
                    return { startPoint: nose, direction: directionVector };
                }
            }
            return null;
        }

         function drawGazeIndicator(gazeData, ctx) {
            const { startPoint, direction } = gazeData;

            const base = {
                x: startPoint.x * canvasElement.width,
                y: startPoint.y * canvasElement.height
            };

            const sensitivity = 3000;
            // ★★★ 修正箇所 ★★★
            const yawToUse = isMirrored ? -direction.x : direction.x;

            const cursor = {
                x: base.x + yawToUse * sensitivity,
                y: base.y + direction.y * sensitivity
            };

            ctx.save();
            if (isMirrored) {
                ctx.translate(canvasElement.width, 0);
                ctx.scale(-1, 1);
            }

            ctx.beginPath();
            ctx.moveTo(base.x, base.y);
            ctx.lineTo(cursor.x, cursor.y);
            ctx.strokeStyle = 'rgba(14, 165, 233, 0.9)';
            ctx.lineWidth = 4;
            ctx.stroke();

            ctx.beginPath();
            ctx.arc(cursor.x, cursor.y, 15, 0, 2 * Math.PI);
            ctx.fillStyle = 'rgba(239, 68, 68, 0.8)';
            ctx.fill();
            ctx.strokeStyle = 'rgba(255, 255, 255, 0.9)';
            ctx.lineWidth = 3;
            ctx.stroke();

            ctx.restore();
        }
        

        // --- MediaPipe Holisticの初期化 ---
        const holistic = new Holistic({
            locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/holistic/${file}`
        });
        holistic.setOptions({
            modelComplexity: 1,
            smoothLandmarks: true,
            refineFaceLandmarks: true,
            minDetectionConfidence: 0.5,
            minTrackingConfidence: 0.5
        });
        holistic.onResults(onResults);
        
        // --- ストリーム制御 ---
        const camera = new Camera(videoElement, {
            onFrame: async () => {
                if(activeStream) await holistic.send({ image: videoElement });
            },
            width: 1280,
            height: 720
        });

        async function startWebcam() {
            stopAllStreams();
            isMirrored = true;
            activeStream = true;
            statusText.textContent = 'Webカメラを起動中...';
            await camera.start();
        }

        function startVideoFile(event) {
            if (!event.target.files || event.target.files.length === 0) return;
            stopAllStreams();
            isMirrored = false;
            activeStream = true;

            const file = event.target.files[0];
            const url = URL.createObjectURL(file);
            videoElement.src = url;
            videoElement.onloadeddata = () => {
                 videoElement.play();
                 sendVideoFrame();
            }
        }
        
        async function sendVideoFrame(){
            if (!activeStream || videoElement.paused || videoElement.ended) {
                if(videoElement.loop && videoElement.ended) {
                    videoElement.play();
                } else {
                    return;
                }
            };
            await holistic.send({image: videoElement});
            requestAnimationFrame(sendVideoFrame);
        }

        function stopAllStreams() {
            activeStream = false;
            videoElement.pause();
            if (videoElement.srcObject){
                videoElement.srcObject.getTracks().forEach(track => track.stop());
            }
            videoElement.srcObject = null;
            videoElement.removeAttribute('src');
        }

        // --- イベントリスナーと初期化 ---
        webcamButton.addEventListener('click', startWebcam);
        videoFileInput.addEventListener('change', startVideoFile);

        (async () => {
            await holistic.initialize();
            statusText.textContent = '準備完了。ソースを選択してください。';
        })();
    </script>
</body>
</html>